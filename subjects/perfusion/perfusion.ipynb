{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from contextlib import contextmanager, nullcontext\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from scripts.helpers import chunk, load_model_from_config\n",
    "from scripts.helpers import sample as advanced_sample\n",
    "\n",
    "\n",
    "def perfusion_t2i(prompt_templates,\n",
    "                  outdir,\n",
    "                  personalized_ckpt,\n",
    "                  step=50,\n",
    "                  ddim_eta=0.0,\n",
    "                  n_iter=1,\n",
    "                  H=512,\n",
    "                  W=512,\n",
    "                  C=4,\n",
    "                  f=8,\n",
    "                  n_samples=4,\n",
    "                  scale=7.5,\n",
    "                  beta=0.7,\n",
    "                  tau=0.15,\n",
    "                  config=\"configs/perfusion_inference.yaml\",\n",
    "                  ckpt=\"./ckpt/v1-5-pruned-emaonly.ckpt\",\n",
    "                  seed=42,\n",
    "                  precision=\"autocast\", # choices=[\"full\", \"autocast\"],\n",
    "                  global_locking=False\n",
    "                ):\n",
    "\n",
    "    assert torch.cuda.is_available()\n",
    "    device = \"cuda\"\n",
    "    batch_size = n_samples\n",
    "    shape = [C, H // f, W // f]\n",
    "\n",
    "    seed_everything(seed)\n",
    "\n",
    "    config = OmegaConf.load(f\"{config}\")\n",
    "    personalized_ckpts = personalized_ckpt.split(',')\n",
    "    n_concepts = len(personalized_ckpts)\n",
    "    if n_concepts > 1:\n",
    "        config.model.target = 'perfusion.perfusion.MultiConceptsPerfusion'\n",
    "        config.model.params.n_concepts = n_concepts\n",
    "    else:\n",
    "        personalized_ckpts = personalized_ckpts[0]\n",
    "\n",
    "    config.model.params.beta = beta\n",
    "    config.model.params.tau = tau\n",
    "    model = load_model_from_config(config, ckpt, personalized_ckpts)\n",
    "    model = model.to(device)\n",
    "\n",
    "    sampler = DDIMSampler(model)\n",
    "\n",
    "    sample = lambda c, uc: (\n",
    "        sampler.sample(\n",
    "            S=step,\n",
    "            conditioning=c,\n",
    "            batch_size=batch_size,\n",
    "            shape=shape,\n",
    "            verbose=False,\n",
    "            unconditional_guidance_scale=scale,\n",
    "            unconditional_conditioning=uc,\n",
    "            eta=ddim_eta,\n",
    "        )[0]\n",
    "    )\n",
    "\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    outpath = outdir\n",
    "\n",
    "    for prompt in prompt_templates:\n",
    "\n",
    "        print(f\"**Prompt**: {prompt}\")\n",
    "\n",
    "        assert prompt is not None\n",
    "        data = [batch_size * [prompt]]\n",
    "\n",
    "        # prompts with placeholder word\n",
    "        placeholders = list(model.embedding_manager.string_to_token_dict.keys())\n",
    "        superclasses = model.embedding_manager.initializer_words\n",
    "        data_concept = list()\n",
    "        data_superclass = list()\n",
    "        for i in range(len(data)):\n",
    "            data_concept.append(list())\n",
    "            data_superclass.append(list())\n",
    "            for j in range(len(data[i])):\n",
    "                prompt_concept, prompt_superclass = data[i][j], data[i][j]\n",
    "                for concept_i in range(n_concepts):\n",
    "                    target = f'{{{concept_i + 1}}}' if n_concepts > 1 else '{}'\n",
    "                    prompt_concept = prompt_concept.replace(target, placeholders[concept_i])\n",
    "                    prompt_superclass = prompt_superclass.replace(target, superclasses[concept_i])\n",
    "                data_concept[i].append(prompt_concept)\n",
    "                data_superclass[i].append(prompt_superclass)\n",
    "\n",
    "        sample_path = os.path.join(outpath, prompt.replace(\"{}\", \"_\"))\n",
    "\n",
    "        os.makedirs(sample_path, exist_ok=True)\n",
    "        base_count = len(os.listdir(sample_path))\n",
    "\n",
    "        precision_scope = autocast if precision == \"autocast\" else nullcontext\n",
    "        with torch.no_grad():\n",
    "            with precision_scope(device):\n",
    "                with model.ema_scope():\n",
    "                    for n in trange(n_iter, desc=\"Sampling\"):\n",
    "                        for data_i in tqdm(range(len(data_concept)), desc=\"data\"):\n",
    "                            prompts = data_concept[data_i]\n",
    "                            prompts_superclass = data_superclass[data_i] if global_locking else None\n",
    "\n",
    "                            uc = None\n",
    "                            if scale != 1.0:\n",
    "                                encoding_uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                                uc = dict(c_crossattn=encoding_uc,\n",
    "                                        c_super=encoding_uc if global_locking else None)\n",
    "                            if isinstance(prompts, tuple):\n",
    "                                prompts = list(prompts)\n",
    "                            encoding = model.cond_stage_model.encode(prompts, embedding_manager=model.embedding_manager)\n",
    "                            encoding_superclass = model.get_learned_conditioning(prompts_superclass) if global_locking else None\n",
    "                            c = dict(c_crossattn=encoding, c_super=encoding_superclass)\n",
    "\n",
    "                            z_samples = sample(c, uc)\n",
    "                            x_samples = model.decode_first_stage(z_samples)\n",
    "                            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                            for x_sample in x_samples:\n",
    "                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                                Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                                    os.path.join(sample_path, f\"{base_count:04d}.jpg\"))\n",
    "                                base_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "import sys\n",
    "utils_path = os.path.abspath(os.path.join('../..'))\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "from utils.dataset_info import get_subjects_prompts_info\n",
    "\n",
    "\n",
    "# Single Subject Generation\n",
    "single_subject = []                            # \"backpack\"\n",
    "# Single Prompt Generation\n",
    "single_prompt = []                            # e.g. [\"a {0} {1} near the pool\"]\n",
    "\n",
    "num_generation = 4\n",
    "\n",
    "\n",
    "output_path = \"../../outputs/subjects/perfusion\"\n",
    "logs_path = \"../../logs/subjects/perfusion/\"\n",
    "subjects = os.listdir(logs_path)\n",
    "dataset_info_path = \"../../pcs_dataset/info.json\"\n",
    "\n",
    "prompts_info = get_subjects_prompts_info(dataset_info_path)\n",
    "\n",
    "if len(single_subject):\n",
    "    subjects = single_subject\n",
    "\n",
    "for subject in subjects:\n",
    "        \n",
    "    print(f\"***** Subject: {subject} *****\")\n",
    "\n",
    "    outdir = os.path.join(output_path, subject)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    if len(single_prompt):\n",
    "        prompts = single_prompt\n",
    "    else:\n",
    "        prompts = prompts_info[subject][\"prompts\"]\n",
    "\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        prompts[idx] = prompt.replace(\"{0} {1}\", \"{}\")\n",
    "    \n",
    "    personalized_ckpt = os.path.join(logs_path, subject, \"models/step=400.ckpt\")\n",
    "    \n",
    "    perfusion_t2i(prompts, outdir, personalized_ckpt)\n",
    "    \n",
    "    print(f\"Finished perfusion in subject: {subject}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perfusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
